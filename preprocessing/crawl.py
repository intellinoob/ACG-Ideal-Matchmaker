import json
import time
import random
import urllib.parse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- å°å…¥ WebDriver Manager ---
try:
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service as ChromeService
    # è¨­ç½® ChromeService ä»¥ä¾¿åœ¨å¾ŒçºŒä½¿ç”¨
    SERVICE = ChromeService(ChromeDriverManager().install())
except ImportError:
    print("Warning: 'webdriver-manager' not installed. Falling back to PATH.")
    SERVICE = None 


# --- é…ç½®èˆ‡æ•¸æ“š ---
BASE_WIKI_URL = "https://zh.moegirl.org.cn"
# ç¢ºä¿å­—ç¬¦åˆ—è¡¨å·²æ˜¯ç¹é«”
CHARACTER_LIST = [
    "äº”æ¢æ‚Ÿ", "ç«ˆé–€ç‚­æ²»éƒ", "é›·å§†(Re:ä»é›¶å¼€å§‹çš„å¼‚ä¸–ç•Œç”Ÿæ´»)", "é»‘å·èŒœ", "èŠ™è‰è“®", "é˜¿å°¼äºšÂ·ç¦æ°", "çŒ«çŒ«(è¯å¸ˆå°‘å¥³çš„ç‹¬è¯­)#", 
    "æœ‰é©¬åŠ å¥ˆ", "è‰¾è‰èÂ·ç±³å“ˆä¼Šç½—èŠ™å¨œÂ·ä¹æ¡", "ç”µæ¬¡(ç”µé”¯äºº)#", "æ—©å·ç§‹", "é­¯è¿ªçƒæ–¯", "èœæœˆæ˜´", "è’™å¥‡Â·DÂ·è·¯é£", 
    "æ¼©æ¸¦é³´äºº", "å­™æ‚Ÿç©º(é¾™ç )#", "æ±Ÿæˆ¶å·æŸ¯å—", "å‚ç”°éŠ€æ™‚", "åˆ©å¨å°”Â·é˜¿å…‹æ›¼", "é­¯è·¯ä¿®Â·è˜­ä½©æ´›åŸº", 
    "é˜¿å°”æ‰˜è‰é›…Â·æ½˜å¾·æ‹‰è´¡", "ç¶¾æ³¢éº—", "èµ¤äº•ç§€ä¸€", "æ¯›åˆ©è˜­", "èµ«è˜¿", "ç™½éŠ€å¾¡è¡Œ", "å››å®®è¼å¤œ", 
    "é›ªä¹‹ä¸‹é›ªä¹ƒ", "åŠ è—¤æƒ (è·¯äººå¥³ä¸»çš„å…»æˆæ–¹æ³•)#", "é¹¿é‡åƒå¤", "å¸æ³¢æ·±é›ª", "åƒçŸ³æ’«å­", "æ¤åçœŸç™½", "å¤ç›®è²´å¿—", 
    "æœˆé‡å…”", "æ¾¤æ‘Â·å²è³“ç‘ŸÂ·è‹±æ¢¨æ¢¨", "æ¹Šé˜¿åº«å©­", "é‡‘æœ¨ç ”", "æœ¨ä¹‹æœ¬æ«»", "å®‡æ™ºæ³¢ä½åŠ©", 
    "å…µè—¤ä¸€èª ", "é˜¿è‰¯è‰¯æœ¨æ›†", "ä¸‰ç¬ Â·é˜¿å…‹æ›¼", "ç´„å…’Â·ä½›å‚‘", "åƒåç”°æ„›ç‘ ", 
    "å’Œæ³‰ç´—éœ§", "æ¡è°·å’Œäºº", "äºçµ²å¨œ", "ç«‹è¯å¥", "è²å€«"
]

# --- ç©©å¥çš„ XPath é¸æ“‡å™¨åˆ—è¡¨ ---
MOE_POINTS_XPATHS = [
    # 1. Flexbox/æ–°æ¨¡æ¿çµæ§‹ (æ‚¨æä¾›çš„çµæ§‹)
    # æŸ¥æ‰¾åŒ…å« 'èŒé»' æˆ– 'èŒç‚¹' çš„ span æ¨™ç±¤ï¼Œç„¶å¾Œé¸å–å…¶å…„å¼Ÿ div 
    "//span[contains(., 'èŒé»') or contains(., 'èŒç‚¹')]/parent::div/following-sibling::div[1]",
    # 2. èˆŠè¡¨æ ¼çµæ§‹ (ä½œç‚ºå‚™ç”¨ Fallback)
    "//td[contains(., 'èŒé»') or contains(., 'èŒç‚¹')]/../td[2]",
    # 3. å‚™ç”¨ Flexbox çµæ§‹ (æ›´é€šç”¨ï¼ŒæŸ¥æ‰¾ç·Šéš¨å…¶å¾Œçš„ç¬¬ä¸€å€‹å…„å¼Ÿå…ƒç´ )
    "//span[contains(., 'èŒé»') or contains(., 'èŒç‚¹')]/following-sibling::*[1]",
]

def clean_moe_points(raw_text):
    """
    å°æå–çš„æ–‡æœ¬é€²è¡Œå¾Œè™•ç†å’Œæ¸…æ´—ï¼Œä»¥ç”Ÿæˆä¹¾æ·¨çš„èŒé»åˆ—è¡¨ã€‚
    """
    if not raw_text:
        return []
        
    # ç§»é™¤è…³è¨»æ¨™è¨˜ï¼Œä¾‹å¦‚ "[3]" æˆ– " [è¨»]"
    import re
    cleaned_text = re.sub(r'\[.*?\]|\s*\[.*?\]|\s*\(.*?\)|(\(\w+\))|\s*(\.\.\.|\(|\))\s*|\s*(\d+)\s*$', '', raw_text)
    
    # èŒé»é€šå¸¸ä»¥ä¸­æ–‡é€—è™Ÿ(ã€)ã€è‹±æ–‡é€—è™Ÿ(,) æˆ–æ›è¡Œç¬¦(\n) åˆ†éš”
    # å°‡æ‰€æœ‰åˆ†éš”ç¬¦çµ±ä¸€æ›¿æ›ç‚ºä¸€å€‹æ¨™æº–åˆ†éš”ç¬¦ (ä¾‹å¦‚: |)ï¼Œç„¶å¾Œåˆ†å‰²
    cleaned_text = cleaned_text.replace('ã€', '|').replace(',', '|').replace('\n', '|')
    
    # åˆ†å‰²ä¸¦éæ¿¾ç©ºå­—ç¬¦ä¸²
    traits = [t.strip() for t in cleaned_text.split('|') if t.strip()]
    
    return traits

def scrape_moe_points(character_name):
    """æ§‹é€  URL ä¸¦å˜—è©¦å¤šå€‹ XPath é¸æ“‡å™¨æå– 'èŒé»'ã€‚"""
    url = f"{BASE_WIKI_URL}/zh-hk/{urllib.parse.quote(character_name)}"
    print(f"\n-> è«‹æ±‚ä¸­: {character_name} ({url})")
    
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless=new")
    options.add_argument("--window-size=1920,1080")

    driver = None
    try:
        driver = webdriver.Chrome(service=SERVICE, options=options) if SERVICE else webdriver.Chrome(options=options)
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        
        for i, xpath in enumerate(MOE_POINTS_XPATHS):
            try:
                text = wait.until(EC.presence_of_element_located((By.XPATH, xpath))).text
                print(f"   [+] æˆåŠŸ! ä½¿ç”¨ XPath #{i+1} æå–èŒé»ã€‚")
                return text
            except TimeoutException:
                continue
        
        print(f"   [!] éŒ¯èª¤: {character_name} è¶…æ™‚ï¼Œæ‰€æœ‰ {len(MOE_POINTS_XPATHS)} å€‹ XPath å‡æœªåŒ¹é…æˆåŠŸã€‚")
        return ""
        
    except WebDriverException as e:
        print(f"   [!] WebDriver é‹è¡ŒéŒ¯èª¤: {e}")
        raise
    except Exception as e:
        print(f"   [!] ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}")
        return ""
    finally:
        if driver:
            driver.quit()

def run_integrated_crawler():
    """ä¸»åŸ·è¡Œå‡½æ•¸ï¼šè¿­ä»£åˆ—è¡¨ï¼ŒåŸ·è¡Œçˆ¬å–ï¼Œä¸¦è¼¸å‡º JSON"""
    database = []
    output_filename = 'character_database.json'
    
    print("--- Selenium èŒå¨˜ç™¾ç§‘çˆ¬èŸ²é–‹å§‹ (å¤š XPath æ¨¡å¼) ---")
    
    for character_name in CHARACTER_LIST:
        try:
            moe_points_raw = scrape_moe_points(character_name)
            
            # --- ç©©å¥æ€§ï¼šå¾Œè™•ç†æ¸…æ´— ---
            traits_list = clean_moe_points(moe_points_raw)
            
            data_entry = {
                "name": character_name,
                "moe_traits": traits_list,
                "trait_count": len(traits_list)
            }
            database.append(data_entry)
            
        except Exception as e:
            # æ•ç²æ‰€æœ‰ç•°å¸¸ï¼Œç¢ºä¿é€²åº¦ä¿å­˜
            print(f"\n[è‡´å‘½éŒ¯èª¤] ç¨‹å¼åœæ­¢ã€‚å·²ä¿å­˜æ•¸æ“šã€‚")
            break 
            
        # è¨­ç½®å»¶é² (Rate Limiting)
        sleep_time = random.uniform(2.0, 4.0)
        time.sleep(sleep_time) 

    # å¯«å…¥ JSON æ–‡ä»¶
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(database, f, ensure_ascii=False, indent=4)
        
    print("\n==============================")
    print(f"ğŸ‰ çˆ¬å–çµæŸã€‚å…±ä¿å­˜ {len(database)} å€‹è§’è‰²çš„æ•¸æ“šã€‚")
    print(f"æ•¸æ“šå·²å„²å­˜è‡³ {output_filename}")
    print("==============================")


if __name__ == "__main__":
    run_integrated_crawler()